{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put everything together in this code\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, Image\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from yaml import ScalarEvent\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm_notebook\n",
    "from itertools import product\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "from ipywidgets import fixed, Box, Layout\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "RANDOM_SEED = np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define all the functions that I will be using\n",
    "\n",
    "### STATISTICS FUNCTIONS ###\n",
    "\n",
    "\n",
    "def build_dataset(dataset):\n",
    "    # function that will help build the required dataset\n",
    "    if dataset == \"Azure CPU Usage\" :\n",
    "        df = pd.read_csv(\"./Azure_Dataset/azure.csv\")\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "        df.drop('min cpu', inplace=True, axis=1)\n",
    "        df.drop('max cpu', inplace=True, axis=1)\n",
    "        print(df.head())\n",
    "        df_value = df[\"avg cpu\"]\n",
    "        x_axis_name = \"Timestamp\"\n",
    "        title = \"Azure Dataset CPU Utilization\"\n",
    "        y_axis_name = \"CPU Utilization [HZ]\"\n",
    "    elif dataset == \"BitBrains\" : \n",
    "        df = pd.read_csv(\"./BitBrains_Dataset/1.csv\", \";\\t\")\n",
    "        df['Timestamp [ms]'] = pd.to_datetime(df['Timestamp [ms]'])\n",
    "        df = df.set_index('Timestamp [ms]')\n",
    "        df.drop('CPU cores', inplace=True, axis=1)\n",
    "        df.drop('CPU usage [%]', inplace=True, axis=1)\n",
    "        df.drop('CPU capacity provisioned [MHZ]', inplace=True, axis=1)\n",
    "        df.drop('Memory capacity provisioned [KB]', inplace=True, axis=1)\n",
    "        df.drop('Memory usage [KB]', inplace=True, axis=1)\n",
    "        df.drop('Disk read throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Disk write throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Network received throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Network transmitted throughput [KB/s]', inplace=True, axis=1)\n",
    "        print(df.head())\n",
    "        df_value = df[\"CPU usage [MHZ]\"]\n",
    "        x_axis_name = \"Timestamp\"\n",
    "        title = \"BitBrains Dataset CPU Utilization\"\n",
    "        y_axis_name = \"CPU Utilization [MHZ]\"      \n",
    "    elif dataset ==  \"KSA\" :\n",
    "        df = pd.read_excel(\"./KSA_Dataset/KSA_test.xlsx\")\n",
    "        df.drop('Response Time', inplace=True, axis=1)\n",
    "        df.drop('Class Name', inplace=True, axis=1) \n",
    "        print(df.head())\n",
    "        df_value = df[\"CPU Utilization\"]\n",
    "        x_axis_name = \"Timestamp\"\n",
    "        title = \"KSA Dataset CPU Utilization\"\n",
    "        y_axis_name = \"CPU Utilization [%]\"\n",
    "    else : \n",
    "        df_value = []\n",
    "        x_axis_name = \"\"\n",
    "        y_axis_name = \"\"\n",
    "        title = \"\"\n",
    "    \n",
    "    \n",
    "    return df_value, x_axis_name, y_axis_name, title\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def eda(df_value, x_axis_name, y_axis_name, title):\n",
    "    plt.figure(figsize=(17, 8))\n",
    "    plt.plot(df_value)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def plot_moving_average(series, window):\n",
    "\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    \n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'r', label='Rolling mean trend')\n",
    "        \n",
    "            \n",
    "    plt.plot(series[window:], alpha=0.5, label='Actual values')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    return 10\n",
    "\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "  \n",
    "def plot_exponential_smoothing(series, alpha):\n",
    " \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "def parser(s, frmat='%Y-%m-%d %H:%M:%S'):\n",
    "    return datetime.strptime(s, frmat)\n",
    "\n",
    "\n",
    "def plot_stl_decomposition(dataset):\n",
    "\n",
    "    \n",
    "    # function that will help build the required dataset\n",
    "    if dataset == \"Azure CPU Usage\" :\n",
    "        df = pd.read_csv(\"./Azure_Dataset/azure.csv\", parse_dates=[0], index_col=0, date_parser=parser)\n",
    "        df = df.asfreq(pd.infer_freq(df.index))\n",
    "        df.drop('min cpu', inplace=True, axis=1)\n",
    "        df.drop('max cpu', inplace=True, axis=1)\n",
    "    elif dataset == \"BitBrains\" : \n",
    "        df = pd.read_csv(\"./BitBrains_Dataset/1.csv\", \";\\t\")\n",
    "        df['Timestamp [ms]'] = pd.to_datetime(df['Timestamp [ms]'])\n",
    "        df = df.set_index('Timestamp [ms]')\n",
    "#         df = df.asfreq(pd.infer_freq(df.index))\n",
    "        df.drop('CPU cores', inplace=True, axis=1)\n",
    "        df.drop('CPU usage [%]', inplace=True, axis=1)\n",
    "        df.drop('CPU capacity provisioned [MHZ]', inplace=True, axis=1)\n",
    "        df.drop('Memory capacity provisioned [KB]', inplace=True, axis=1)\n",
    "        df.drop('Memory usage [KB]', inplace=True, axis=1)\n",
    "        df.drop('Disk read throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Disk write throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Network received throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Network transmitted throughput [KB/s]', inplace=True, axis=1)\n",
    "      \n",
    "    else : \n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.rc('figure',figsize=(14,8))\n",
    "    plt.rc('font',size=15)\n",
    "\n",
    "    result = seasonal_decompose(df,model='additive', period = 60*24)\n",
    "    fig = result.plot()  \n",
    "    \n",
    "\n",
    "def tsplot(y, lags=None):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=(12,7))\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "\n",
    "### SARIMA ###\n",
    "\n",
    "# Train many SARIMA models to find the best set of parameters\n",
    "\n",
    "def optimize_SARIMA(parameters_list, d, D, s, data):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        \n",
    "        parameters_list - list with (p, q, P, Q) tuples\n",
    "        d - integration order\n",
    "        D - seasonal integration order\n",
    "        s - length of season\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float('inf')\n",
    "    \n",
    "    for param in tqdm_notebook(parameters_list):\n",
    "        try: model = sm.tsa.statespace.SARIMAX(data, order=(param[0], d, param[1]),\n",
    "                                               seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        \n",
    "        #Save best model, AIC and parameters\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "        \n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table\n",
    "\n",
    "\n",
    "def train_SARIMA(d, D, s, data):\n",
    "    #Set initial values and some bounds\n",
    "    ps = range(0, 2)\n",
    "    d = d\n",
    "    qs = range(0, 2)\n",
    "    Ps = range(0, 2)\n",
    "    D = D\n",
    "    Qs = range(0, 2)\n",
    "    s = s\n",
    "\n",
    "    #Create a list with all possible combinations of parameters\n",
    "    parameters = product(ps, qs, Ps, Qs)\n",
    "    parameters_list = list(parameters)\n",
    "    len(parameters_list)\n",
    "\n",
    "    result_table = optimize_SARIMA(parameters_list, d, D, s, data)\n",
    "\n",
    "    #Set parameters that give the lowest AIC (Akaike Information Criteria)\n",
    "    p, q, P, Q = result_table.parameters[0]\n",
    "\n",
    "    best_model = sm.tsa.statespace.SARIMAX(data, order=(p, d, q),\n",
    "                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)\n",
    "\n",
    "    print(best_model.summary())\n",
    "\n",
    "\n",
    "def sarima(dataset):\n",
    "    df_value, x_axis_name, y_axis_name, title = build_dataset(dataset)\n",
    "    if x_axis_name == \"\" :\n",
    "        return None\n",
    "    sarima = SARIMAX(df_value, \n",
    "                order=(1,1,1), \n",
    "                seasonal_order=(1,1,0,12))\n",
    "    print(\"training started ...\")\n",
    "    predictions = sarima.fit().predict()\n",
    "\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(df_value, label=\"Actual\")\n",
    "    plt.plot(predictions, label=\"Predicted\")\n",
    "    plt.title('SARIMA prediction on selected dataset', fontsize=20)\n",
    "    plt.ylabel('CPU Usage', fontsize=16)\n",
    "    plt.legend()    \n",
    "    plt.axis('tight')\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    testScore_1 = math.sqrt(mean_squared_error(df_value[:], predictions[:]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore_1))\n",
    "\n",
    "    testScore_2 = math.sqrt(mean_absolute_error(df_value[:], predictions[:]))\n",
    "    print('Test Score: %f MAE' % (testScore_2))\n",
    "\n",
    "    testScore_3 = np.mean(np.abs(predictions - df_value)/np.abs(df_value)*100)\n",
    "    print('Test Score: %f MAPE' % (testScore_3))\n",
    "\n",
    "### SARIMAOVER ###\n",
    "\n",
    "def plot_statistics(dataset):\n",
    "    df_value, x_axis_name, y_axis_name, title = build_dataset(dataset)\n",
    "    if x_axis_name == \"\" :\n",
    "        return None\n",
    "    eda(df_value, x_axis_name, y_axis_name, title)\n",
    "\n",
    "    w = widgets.interactive(plot_moving_average, series=fixed(df_value), window=window_slider)\n",
    "    expo_smooth = widgets.interactive(plot_exponential_smoothing, series=fixed(df_value), alpha=alpha_slider)\n",
    "    ts_plot = widgets.interactive(tsplot, y=fixed(df_value), lags=lags_slider)\n",
    "    \n",
    "    plot_stl_decomposition(dataset)\n",
    "    \n",
    "    VBOX = widgets.VBox(children=[w, expo_smooth, ts_plot], titles=('Moving Average', 'Exponential Smoothing', 'TSPLOT'))\n",
    "    display(VBOX)\n",
    "    return df_value\n",
    "    \n",
    "def f(a,b,c):\n",
    "    return None \n",
    "\n",
    "### STATISTICS FUNCTIONS OVER ###\n",
    "\n",
    "### ML FUNCTIONS ###\n",
    "\n",
    "def train_generator(dataset, n_lags=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - n_lags - 1):\n",
    "        a = dataset.iloc[i:(i+n_lags)].to_numpy()\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset.iloc[i + n_lags].to_numpy())\n",
    "\n",
    "    dataX = np.array(dataX)\n",
    "    dataY = np.array(dataY)\n",
    "\n",
    "    new_dataX = np.empty([dataX.shape[0], dataX.shape[1]])\n",
    "    for i in range(len(dataX)) : \n",
    "        for j in range(dataX.shape[1]) :\n",
    "            new_dataX[i][j] = dataX[i][j][0]\n",
    "\n",
    "    new_dataY = np.empty(dataY.shape[0])\n",
    "    for i in range(len(dataY)) : \n",
    "        new_dataY[i] = dataY[i][0]\n",
    "\n",
    "    return (np.array(new_dataX), np.array(new_dataY))\n",
    "\n",
    "\n",
    "def pre_process(dataset_name, timesteps, train_test_split) :\n",
    "    # let's load the datasets\n",
    "    if dataset_name == \"Azure CPU Usage\" :\n",
    "        df = pd.read_csv(\"./Azure_Dataset/azure.csv\")\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "        df.drop('min cpu', inplace=True, axis=1)\n",
    "        df.drop('max cpu', inplace=True, axis=1)\n",
    "\n",
    "    elif dataset_name == \"BitBrains\" : \n",
    "        df = pd.read_csv(\"./BitBrains_Dataset/1.csv\", \";\\t\")\n",
    "        df['Timestamp [ms]'] = pd.to_datetime(df['Timestamp [ms]'])\n",
    "        df = df.set_index('Timestamp [ms]')\n",
    "        df.drop('CPU cores', inplace=True, axis=1)\n",
    "        df.drop('CPU usage [%]', inplace=True, axis=1)\n",
    "        df.drop('CPU capacity provisioned [MHZ]', inplace=True, axis=1)\n",
    "        df.drop('Memory capacity provisioned [KB]', inplace=True, axis=1)\n",
    "        df.drop('Memory usage [KB]', inplace=True, axis=1)\n",
    "        df.drop('Disk read throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Disk write throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Network received throughput [KB/s]', inplace=True, axis=1)\n",
    "        df.drop('Network transmitted throughput [KB/s]', inplace=True, axis=1)\n",
    "\n",
    "    elif dataset_name ==  \"KSA\" :\n",
    "        df = pd.read_excel(\"./KSA_Dataset/KSA_test.xlsx\")\n",
    "        df.drop('Response Time', inplace=True, axis=1)\n",
    "        df.drop('Class Name', inplace=True, axis=1) \n",
    "    \n",
    "    else : \n",
    "        print(\"Wrong dataset name, please chose from : azure, bit brains and KSA\")\n",
    "\n",
    "    # create train test split\n",
    "    train_length = round(len(df)*train_test_split)\n",
    "    test_length = len(df) - train_length\n",
    "    train = df.iloc[0:train_length]\n",
    "    test = df.iloc[train_length:]\n",
    "\n",
    "    train_mean = train.mean()\n",
    "    train_std = train.std()\n",
    "\n",
    "    train = (train - train_mean) / train_std\n",
    "    test = (test - train_mean) / train_std\n",
    "\n",
    "    # let's scale the values of the dataset\n",
    "    scaler = MinMaxScaler(feature_range = (0,1)) #transform features by scaling each feature to a given range\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train), columns=df.columns)\n",
    "    test_scaled = pd.DataFrame(scaler.fit_transform(test), columns=df.columns)\n",
    "\n",
    "\n",
    "    X_train, y_train = train_generator(train_scaled, n_lags = timesteps)\n",
    "    X_test_scaled, y_test_scaled = train_generator(test_scaled, n_lags=timesteps)\n",
    "    X_test, y_test = train_generator(test, n_lags=timesteps)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_test_scaled, y_test_scaled, scaler\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes as input : \n",
    "- the models that we want to train (3 options)\n",
    "- the parameters of the training\n",
    "- ouputs training results and graphs\n",
    "\n",
    "\"\"\"\n",
    "def train_models(list_of_models=[], print_summary=False, X_train=[], \n",
    "                    y_train=[], loss_function=\"mean_absolute_error\", \n",
    "                    optimizer=tf.keras.optimizers.Adam(), epochs=200, validation_split=0.25,\n",
    "                    batch_size=256, verbose=1, save__model_path=\"\") :\n",
    "\n",
    "    for model in list_of_models :\n",
    "        if model not in [\"NN\", \"GRU\", \"LSTM\"] :\n",
    "            print(\"Model should be between the following : NN, GRU, LSTM\")   \n",
    "            return None\n",
    "        if model == \"NN\" :\n",
    "            # build a NN and train it\n",
    "            model_NN = tf.keras.Sequential()\n",
    "            model_NN.add(tf.keras.layers.Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "            model_NN.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "            model_NN.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            if print_summary == True : \n",
    "                print(model_NN.summary())\n",
    "            \n",
    "            model_NN.compile(loss=loss_function, optimizer=optimizer)\n",
    "            es = tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)\n",
    "            lr_red = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, min_lr=0.0000001,)\n",
    "\n",
    "\n",
    "            callbacks = [es, lr_red]\n",
    "            history = model_NN.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size, verbose=verbose,\n",
    "                                shuffle=False,\n",
    "                                callbacks = callbacks)\n",
    "            \n",
    "            if save__model_path != \"\" :\n",
    "                model_NN.save(save__model_path)\n",
    "\n",
    "            # let's print some graphs\n",
    "\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            # why not also draw out the learning rate\n",
    "            plt.plot(history.history['lr'])\n",
    "            plt.title('Learning Rate')\n",
    "            plt.ylabel('LR')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.show()\n",
    "\n",
    "            return model_NN\n",
    "\n",
    "        if model == \"GRU\" : \n",
    "            # build an GRU and train it\n",
    "            model_GRU = tf.keras.models.Sequential()\n",
    "            model_GRU.add(tf.keras.layers.GRU(512,input_shape=(X_train.shape[1], 1),return_sequences=True))\n",
    "            model_GRU.add(tf.keras.layers.GRU(512, return_sequences=False))\n",
    "            model_GRU.add(tf.keras.layers.Dense(1))\n",
    "            model_GRU.summary()\n",
    "            \n",
    "            if print_summary == True : \n",
    "                print(model_GRU.summary())\n",
    "            \n",
    "            model_GRU.compile(loss=loss_function, optimizer=optimizer)\n",
    "            es = tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)\n",
    "            lr_red = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, min_lr=0.0000001,)\n",
    "\n",
    "\n",
    "            callbacks = [es, lr_red]\n",
    "            history = model_GRU.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size, verbose=verbose,\n",
    "                                shuffle=False,\n",
    "                                callbacks = callbacks)\n",
    "            \n",
    "            if save__model_path != \"\" :\n",
    "                model_GRU.save(save__model_path)\n",
    "\n",
    "            # let's print some graphs\n",
    "\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            # why not also draw out the learning rate\n",
    "            plt.plot(history.history['lr'])\n",
    "            plt.title('Learning Rate')\n",
    "            plt.ylabel('LR')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.show()\n",
    "\n",
    "            return model_GRU\n",
    "\n",
    "        if model == \"LSTM\" :\n",
    "            # build an LSTM and train it\n",
    "            model_lstm = tf.keras.models.Sequential()\n",
    "            model_lstm.add(tf.keras.layers.LSTM(512,input_shape=(X_train.shape[1], 1),return_sequences=True))\n",
    "            model_lstm.add(tf.keras.layers.LSTM(512, return_sequences=False))\n",
    "            model_lstm.add(tf.keras.layers.Dense(1))\n",
    "            model_lstm.summary()\n",
    "            \n",
    "            if print_summary == True : \n",
    "                print(model_lstm.summary())\n",
    "            \n",
    "            model_lstm.compile(loss=loss_function, optimizer=optimizer)\n",
    "            es = tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)\n",
    "            lr_red = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, min_lr=0.0000001,)\n",
    "\n",
    "\n",
    "            callbacks = [es, lr_red]\n",
    "            history = model_lstm.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size, verbose=verbose,\n",
    "                                shuffle=False,\n",
    "                                callbacks = callbacks)\n",
    "            \n",
    "            if save__model_path != \"\" :\n",
    "                model_lstm.save(save__model_path)\n",
    "\n",
    "            # let's print some graphs\n",
    "\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            # why not also draw out the learning rate\n",
    "            plt.plot(history.history['lr'])\n",
    "            plt.title('Learning Rate')\n",
    "            plt.ylabel('LR')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.show()\n",
    "\n",
    "            return model_lstm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes as input : \n",
    "- the model\n",
    "- the test set\n",
    "- outputs the test scores and the graphs we want\n",
    "\"\"\"\n",
    "def test_models(model, y_test, X_test_scaled, scaler):\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    preds = scaler.inverse_transform(preds)\n",
    "    plt.rcParams[\"figure.figsize\"] = (32,12)\n",
    "    TestY = pd.DataFrame(y_test, columns=['avg_cpu'])\n",
    "    PredY = pd.DataFrame(preds, columns=['avg_cpu'])\n",
    "\n",
    "    plot_avg = plt.figure(1)\n",
    "    plt.plot(TestY['avg_cpu'])\n",
    "    plt.plot(PredY['avg_cpu'])\n",
    "    plt.show()\n",
    "\n",
    "    testScore_1 = math.sqrt(mean_squared_error(y_test[:], preds[:]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore_1))\n",
    "\n",
    "    testScore_2 = math.sqrt(mean_absolute_error(y_test[:], preds[:]))\n",
    "    print('Test Score: %f MAE' % (testScore_2))\n",
    "\n",
    "    testScore_3 = np.mean(np.abs(preds - y_test)/np.abs(y_test)*100)\n",
    "    print('Test Score: %f MAPE' % (testScore_3))\n",
    "\n",
    "\n",
    "def plot_ml(dataset, n_lags, model_name, print_summary, loss_function, epochs):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, X_test_scaled, y_test_scaled, scaler = pre_process(dataset, n_lags, 0.8)\n",
    "\n",
    "    # let's train the models\n",
    "    model = train_models([model_name], print_summary, X_train, y_train, loss_function, \n",
    "                                    tf.keras.optimizers.Adam(), epochs, 0.25, 256, 1, \"\")\n",
    "    model.save(\"./model_\"+model_name)\n",
    "\n",
    "    test_models(model, y_test, X_test_scaled, scaler)\n",
    "\n",
    "\n",
    "### ML FUNCTIONS OVER ###\n",
    "\n",
    "def choose_vars(dataset, n_lags, model_name, print_summary, loss_function, epochs):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create all my widgest here : \n",
    "\n",
    "dataset_selection = widgets.Dropdown(\n",
    "    options=[' ', 'Azure CPU Usage', 'BitBrains', 'KSA'],\n",
    "    value=' ',\n",
    "    description='Choose the dataset from the following list :',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "dataset_selection_ml = widgets.Dropdown(\n",
    "    options=['Azure CPU Usage', 'BitBrains', 'KSA'],\n",
    "    value='KSA',\n",
    "    description='Choose the dataset from the following list :',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "dataset_selection_sarima = widgets.Dropdown(\n",
    "    options=[' ', 'Azure CPU Usage', 'BitBrains', 'KSA'],\n",
    "    value=' ',\n",
    "    description='Choose the dataset from the following list :',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "window_slider = widgets.IntSlider(\n",
    "    value=60,\n",
    "    min=0,\n",
    "    max=200,\n",
    "    step=20,\n",
    "    description='Window size for moving average computation',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=Layout(width='60%', height='100px')\n",
    ")\n",
    "\n",
    "lags_slider = widgets.IntSlider(\n",
    "    value=30,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=10,\n",
    "    description='Lags',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='d',\n",
    "    layout=Layout(width='60%', height='100px')\n",
    ")\n",
    "\n",
    "alpha_slider = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description='Alpha parameter for exponential smoothing',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='d',\n",
    "    layout=Layout(width='60%', height='100px')\n",
    ")\n",
    "\n",
    "sarima_d_drpdwn = widgets.Dropdown(\n",
    "    options=['1', '2', '3', '4', '5'],\n",
    "    value='2',\n",
    "    description='d value :',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "sarima_D_drpdwn = widgets.Dropdown(\n",
    "    options=['1', '2', '3', '4', '5'],\n",
    "    value='2',\n",
    "    description='D value :',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "sarima_s_drpdwn = widgets.Dropdown(\n",
    "    options=['1', '2', '3', '4', '5'],\n",
    "    value='2',\n",
    "    description='s value :',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "model_drpdown = widgets.Dropdown(\n",
    "    options=['NN', 'GRU', 'LSTM'],\n",
    "    value='NN',\n",
    "    description='Choose model :',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "print_summary_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Print Summary',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "loss_function_drpdown = widgets.Dropdown(\n",
    "    options=['mean_absolute_error'],\n",
    "    value='mean_absolute_error',\n",
    "    description='Choose loss function :',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "epochs_number = widgets.Text(\n",
    "    value='200',\n",
    "    placeholder='Type something',\n",
    "    description='Epochs :',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "n_lags_slider = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=10,\n",
    "    description='Window size for dataset building',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='d'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTM FY22 : IT Capacity Planning\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "\n",
    "The goal of this project is to complete a study that will showcase the use of analytics and AI in order to forecast IT servers usage, as well as potentially predict capacity needs.\n",
    "\n",
    "Our study was done on three different datasets from three different sources : \n",
    "\n",
    "1. Microsoft Azure : this first dataset contains the CPU Usage data of machines running Microsoft Azure services, sampled every 5 minutes. The data has three attributes : \n",
    "    - Max CPU Utilization\n",
    "    - Average CPU Utilization\n",
    "    - Minimum CPU Utilization\n",
    "    \n",
    "    In this study, we only focused on the average CPU Utilization, but of course this could be extended to any kind of usage. A further study could also be implemented by using these three values in order to predict a single futuristic value\n",
    "\n",
    "<img src=\"Images/azure.png\" alt=\"Azure Dataset\" style=\"height: 100px; width:180px;\"/>\n",
    "\n",
    "2. GWA-T-12 Bitbrains : the dataset contains the performance metrics of 1,750 VMs from a distributed datacenter from Bitbrains, which is a service provider that specializes in managed hosting and business computation for enterprises. Customers include many major banks (ING), credit card operators (ICS), insurers (Aegon), etc. Bitbrains hosts applications used in the solvency domain; examples of application vendors are Towers Watson and Algorithmics. These applications are typically used for financial reporting, which is used predominately at the end of financial quarters.\n",
    "\n",
    "    In this study, we only focus on the results of single machines. So typically, a prediction model is built for one single machine, since different machines coulnd be running different programs, with different configurations ... and these are not mentioned in the dataset. But the following was also tested : training a model on a specific machine and predicting the workload of another one. This also worked, not to the best results, but it could definetly be an interesting future field of work\n",
    "\n",
    "<img src=\"Images/TUDLogo.png\" alt=\"TU Delft\" style=\"height: 100px; width:200px;\"/>\n",
    "\n",
    "- Here are the metrics measured in the dataset (in this study we only use and focus on the CPU usage in terms of MHZ) : \n",
    "    - Timestamp: number of milliseconds since 1970-01-01.\n",
    "    - CPU cores: number of virtual CPU cores provisioned.\n",
    "    - CPU capacity provisioned (CPU requested): the capacity of the CPUs in terms of MHZ, it equals to number of cores x speed per core.\n",
    "    - CPU usage: in terms of MHZ.\n",
    "    - CPU usage: in terms of percentage\n",
    "    - Memory provisioned (memory requested): the capacity of the memory of the VM in terms of KB.\n",
    "    - Memory usage: the memory that is actively used in terms of KB.\n",
    "    - Disk read throughput: in terms of KB/s\n",
    "    - Disk write throughput: in terms of KB/s\n",
    "    - Network received throughput: in terms of KB/s\n",
    "    - Network transmitted throughput: in terms of KB/s\n",
    "\n",
    "3. KSA Ministry of Finance : this dataset was collected from the KSA Ministry of Finance that contains 28,147 instances from 13 cloud nodes. It was recorded during the period from March 1, 2016, to February 20, 2017, in continuous time slots. The data represent the performance of the servers implemented in the institution. The follwoing metrics were measured : \n",
    "    - Number of Jobs in a minute/5min/15min\n",
    "    - Memory Capacity\n",
    "    - Disk Capacity\n",
    "    - Number of CPU Cores\n",
    "    - CPU speed per Core\n",
    "    - Average receive for network bandwidth in Kbps\n",
    "    - Average transmit for network bandwidth in Kbps\n",
    "    - Memory utilization in percent\n",
    "    - CPU utilization in percent\n",
    "    - Response Time in milliseconds\n",
    "\n",
    "    It is clear that a lot of metrics can be useful for this study. Here, we only focused on the CPU as it is the metric we decided to build our study around. But of course, this could be definetly completed by an inside-out prediction : for example using the external parameters (number of jobs, mem capacity, network bandwidth ...) to predict the inner values (CPU,Memory utilization) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "In this part, you can choose a dataset from the dropdown below and different graphs will appear to showcase data and metadata about the dataset.\n",
    "\n",
    "Before jumping into these results, let us have an overlook on the definition of the below graphs/metric : \n",
    "\n",
    "1. An overview of the data (couple of rows and a graph representation)\n",
    "\n",
    "2. Moving Average : In statistics, a moving average is a calculation used to analyze data points by creating a series of averages of different subsets of the full data set (of a fixed window size)\n",
    "\n",
    "3. Exponential Smoothing : Exponential smoothing is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time\n",
    "\n",
    "4. Dickey-Fuller test : Named for American statisticians David Dickey and Wayne Fuller, who developed the test in 1979, the Dickey-Fuller test is used to determine whether a unit root (a feature that can cause issues in statistical inference) is present in an autoregressive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b1bd5ddde440b898ab84a02eb36c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Choose the dataset from the following list :', options=(' ', 'Azur…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stats = widgets.interactive(plot_statistics, dataset=dataset_selection)\n",
    "display(plot_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA\n",
    "\n",
    "In this section, the SARIMA prediction algorithm is implemented.\n",
    "\n",
    "Let's go over the theoretical part of ARIMA and SARIMA : \n",
    "\n",
    "- ARIMA : ARIMA model is a class of linear models that utilizes historical values to forecast future values. ARIMA stands for Autoregressive Integrated Moving Average, each of which technique contributes to the final forecast. Let’s understand it one by one.\n",
    "    - Autoregressive : we forecast the variable of interest using a linear combination of past values of that variable\n",
    "    - Integrated : represents any differencing that has to be applied in order to make the data stationary\n",
    "    - Moving Average : Moving average models uses past forecast errors rather than past values in a regression-like model to forecast future values\n",
    " \n",
    "\n",
    "- SARIMA : SARIMA stands for Seasonal-ARIMA and it includes seasonality contribution to the forecast. The importance of seasonality is quite evident and ARIMA fails to encapsulate that information implicitly. The Autoregressive (AR), Integrated (I), and Moving Average (MA) parts of the model remain as that of ARIMA. The addition of Seasonality adds robustness to the SARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a41de1656b40f88992b2461b0d0f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Choose the dataset from the following list :', options=(' ', 'Azur…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sarima = widgets.interactive(sarima, dataset=dataset_selection_sarima)\n",
    "display(plot_sarima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "In this section, we implemented everything related to ML in this study. You can first choose a dataset below, the model that you would like to train on this dataset as well as different parameters.\n",
    "\n",
    "The training and testing results/metrics will be printed out then!\n",
    "\n",
    "For results metric, we use : \n",
    "- RMSE : Root Mean Squared Error is the square root of Mean Squared Error (MSE). MSE is nothing but a representation of how forecasted values differ from actual or true ones. We take the square root in order to avoid the negative sign as errors can be positive or negative\n",
    "\n",
    "\n",
    "- MAPE : Mean Absolute Percentage Error is the measure of how accurate a forecast system is. It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values\n",
    "\n",
    "\n",
    "- MAE : The MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. The equation is given in the library references. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. The MAE is a linear score which means that all the individual differences are weighted equally in the average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103e79525aa247c58a490f7597c2101d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Choose the dataset from the following list :', index=2, options=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "choose_vars = widgets.interactive(choose_vars, dataset=dataset_selection_ml, model_name=model_drpdown, n_lags=n_lags_slider, \n",
    "                            print_summary=print_summary_checkbox, loss_function=loss_function_drpdown, \n",
    "                            epochs=epochs_number)\n",
    "display(choose_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a091056a5b324f35869cb0bb5e0a57b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click here to launch ML', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dff523846384dd9b563665d98b512fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_ml = choose_vars.children[0].value\n",
    "n_lags = choose_vars.children[1].value\n",
    "print_summary = choose_vars.children[3].value\n",
    "loss_function = choose_vars.children[4].value\n",
    "epochs = int(choose_vars.children[5].value)\n",
    "model_name = choose_vars.children[2].value\n",
    "\n",
    "button = widgets.Button(description=\"Click here to launch ML\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(button, output)\n",
    "\n",
    "def launch_ML(b):\n",
    "    with output:\n",
    "        plot_ml(dataset_ml, n_lags, model_name, print_summary, loss_function, epochs)\n",
    "button.on_click(launch_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e530692e177f3bd98b0c8d10b5be8832015fb3946aed28d23528612d12c22b23"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
